q()
q()
packages <- c("R.matlab", "tsibble", "cowplot", "tidyverse", "zoo")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
# Packages loading""
invisible(lapply(packages, library, character.only = TRUE)) # GBM
# configuration variables
G_sample_frequency = 187
G_debug = FALSE
setwd("/home/msolerortiz/workspace_R/SR_stats_lightning")
######### FUNCTIONS ####
get_datetime_from_file <- function(file) {
raw_info = unlist( strsplit(file, "_") )
file_date = paste(raw_info[2],raw_info[3],raw_info[4], sep="/")
file_datetime = paste(file_date, raw_info[5], sep=" ")
#file_name is arrival date, 30 minutes shifting to get start date.
date_info = ymd_hms(file_datetime) - minutes(30)
return(date_info)
}
char_and_add_zero <-function(value) {
if (value > 9) {
value_as_char = as.character(value)
}
else {
value_as_char = paste( "0", as.character(value), sep="" )
}
return(value_as_char)
}
#using year-month-day-hour we can find the files where the data sought might be.
#this function creates
get_search_pattern_from_datetime <- function(date) {
#file_name is arrival date, date must be shifted backwards 30 minutes.
date = date + minutes(30)
yr = as.character( year(date) )
mo = char_and_add_zero( month(date) )
dy = char_and_add_zero( day(date) )
ho = char_and_add_zero( hour(date) )
file_pattern = paste("c", yr, mo, dy, ho, sep="_")
return(file_pattern)
}
get_filename_from_datetime <- function(date, or) {
#file_name is arrival date, date must be shifted backwards 30 minutes.
date = date + minutes(30)
yr = as.character( year(date) )
mo = char_and_add_zero( month(date) )
dy = char_and_add_zero( day(date) )
ho = char_and_add_zero( hour(date) )
mi = char_and_add_zero( minute(date) )
se = char_and_add_zero( second(date) )
file_time = paste(ho,mi,se, sep = "")
file_date = paste("c", yr, mo, dy, file_time, or, "cal.mat", sep="_")
return(file_date)
}
date_difference_as_samples <- function(date_post, date_prev) {
seconds_diff = as.numeric(date_post - date_prev, units = "secs")
samples_diff = duration_to_samples(seconds_diff)
return(samples_diff)
}
retrieve_file_list_by_orientation <- function(file_path, orientation) {
search_path = dir( file_path )
is_oriented = str_detect(search_path, orientation)
is_filtered = str_detect(search_path, "fil")
return( search_path[is_oriented & is_filtered] )
}
load_SR_data <- function(data_path, file) {
data <- unlist( readMat( paste( data_path, file, sep="/" ) ), use.names = FALSE )
return( data )
}
load_matlab_as_tsibble <- function(data_path) {
#ToDo: remove the hardcoded 60 seconds to work with segments with different separation.
#read .mat data into an R tsibble
matlabFile <- readMat(data_path)
dts <- dates("01/01/2016")
tms <- times(c("00:00:00"))
start_time <- chron(dates = dts, times = tms)
data_frame <- tsibble(
time = as_datetime(as.numeric(matlabFile$index) * 60, origin = start_time),
nor = as.numeric(matlabFile$is.nor),
lap = as.numeric(matlabFile$is.lap),
log = as.numeric(matlabFile$is.log),
avg = as.numeric(matlabFile$mean),
std = as.numeric(matlabFile$std),
skew = as.numeric(matlabFile$skew),
kurt = as.numeric(matlabFile$kurt),
index = time
)
data_frame = data_frame %>%
fill_gaps( .full = TRUE)
rm(matlabFile)
return(data_frame)
}
mean_na_handling <- function(values, threshold){
#calculates the mean of the array given as values if the % of na is under threshold
na_ind = is.na( values )
na_ratio = sum( na_ind ) / length(values)
if ( na_ratio <= threshold ) {
mean_value = mean(values[ !na_ind ])
} else {
mean_value = NA
}
return(mean_value)
}
ma_with_na_handling <- function(values, threshold, w_before, w_after) {
#moving average using mean_na_handling; interpolates linearly any na elements
#left after the filtering.
averaged = slider::slide_dbl(values, mean, .before = w_before, .after = w_after,
.complete = FALSE, .size = length( values ) )
na_ind = which( is.na(averaged) )
for (val in na_ind) {
start_val = val - w_before;
end_val = val + w_after;
if ( end_val > length(values) ) {
end_val = length(values)
}
if ( start_val >= 1 ) {
averaged[val] = mean_na_handling( values[ start_val:end_val ], threshold )
}
}
averaged = na.approx(averaged, na.rm = FALSE)
return(averaged)
}
duration_to_samples <- function(duration) {
return(duration * G_sample_frequency)
}
save_fig <- function(name, type) {
#aliases for ggsave in different sizes.
switch(type,
wide = ggsave(name, device= "png", path = getwd(),
width = 21, height = 12, units = "cm"),
double_wide = ggsave(name, device= "png", path = getwd(),
width = 21, height = 10, units = "cm"),
square1 = ggsave(name, device= "png", path = getwd(),
width = 11, height = 9, units = "cm"),
square2 = ggsave(name, device= "png", path = getwd(),
width = 9, height = 9, units = "cm"),
slim = ggsave(name, device= "png", path = getwd(),
width = 21, height = 4, units = "cm")
)
}
######### DATA CONFIG  ####
na_ratio_threshold = 0.75
my_color_scale = c("springgreen", "lightskyblue", "darkolivegreen4", "lightskyblue4", "darkgreen", "darkblue")
#years_color_scale = c("gold2", "goldenrod",  "gold4", "saddlebrown", "bisque4")
years_color_scale = c("#F8766D", "#00B9E3", "#D39200", "#800080", "#00BA38")
my_elements = c("mnor", "cnor", "mmonor", "cmonor", "myenor", "cyenor")
my_labels = c("10 min raw", "1 min raw", "10 min\nmonthly", "1 min\nmonthly", "10 min\nseasonal", "1 min\nseasonal")
lagged_significant_days = c(91, 182, 274, 365, 456, 547, 639, 730)
file1minEW_path = paste(getwd(),"S_EW_1min.mat", sep ="/")
ts_1min_EW = load_matlab_as_tsibble(file1minEW_path)
packages <- c("R.matlab", "tsibble", "cowplot", "tidyverse", "zoo", "lubridate")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
# Packages loading""
invisible(lapply(packages, library, character.only = TRUE)) # GBM
file1minEW_path = paste(getwd(),"S_EW_1min.mat", sep ="/")
ts_1min_EW = load_matlab_as_tsibble(file1minEW_path)
??dates
packages <- c("R.matlab", "tsibble", "cowplot", "tidyverse", "zoo", "lubridate",
"chron")
# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
# Packages loading""
invisible(lapply(packages, library, character.only = TRUE)) # GBM
file1minEW_path = paste(getwd(),"S_EW_1min.mat", sep ="/")
ts_1min_EW = load_matlab_as_tsibble(file1minEW_path)
file10minEW_path = paste(getwd(),"S_EW_10min.mat", sep ="/")
ts_10min_EW = load_matlab_as_tsibble(file10minEW_path)
file1minNS_path = paste(getwd(),"S_NS_1min.mat", sep ="/")
ts_1min_NS = load_matlab_as_tsibble(file1minNS_path)
file10minNS_path = paste(getwd(),"S_NS_10min.mat", sep ="/")
ts_10min_NS = load_matlab_as_tsibble(file10minNS_path)
######### Hourly averaged by year
ts_year_hours_EW = ts_month_hours_EW %>%
index_by(year_t = lubridate::year(Datetime) ) %>%
mutate(hour_t = hour) %>%
group_by(year_t, hour_t) %>%
summarize( across(everything(), ~ mean_na_handling(.x, na_ratio_threshold) ) ) %>%
as_tsibble(key = year_t, index = hour_t) %>%
select(hour_t,year_t,nor,lap,log)
######### Hourly averaged by month
# Data frame
ts_month_hours_EW = ts_1min_EW %>%
index_by(time_h = ~ lubridate::ceiling_date(., "hour") ) %>%
summarize( across(everything(), ~ mean_na_handling(.x, na_ratio_threshold) ) ) %>%
index_by(hour = ~ lubridate::hour(.) ) %>%
mutate(Datetime = lubridate::floor_date(time_h, "month")) %>%
mutate(Datetime = yearmonth(Datetime)) %>%
group_by(Datetime, hour) %>%
summarize( across(everything(), ~ mean_na_handling(.x, na_ratio_threshold) ) ) %>%
as_tsibble(key=Datetime, index=hour) %>%
select(Datetime, hour, nor, lap, log) %>%
filter(Datetime < yearmonth("2021-01-01") )
ts_month_hours_NS = ts_1min_NS %>%
index_by(time_h = ~ lubridate::ceiling_date(., "hour") ) %>%
summarize( across(everything(), ~ mean_na_handling(.x, na_ratio_threshold) ) ) %>%
index_by(hour = ~ lubridate::hour(.) ) %>%
mutate(Datetime = lubridate::floor_date(time_h, "month")) %>%
mutate(Datetime = yearmonth(Datetime)) %>%
group_by(Datetime, hour) %>%
summarize( across(everything(), ~ mean_na_handling(.x, na_ratio_threshold) ) ) %>%
as_tsibble(key=Datetime, index=hour) %>%
select(Datetime, hour, nor, lap, log) %>%
filter(Datetime < yearmonth("2021-01-01") )
######### Hourly averaged by year
ts_year_hours_EW = ts_month_hours_EW %>%
index_by(year_t = lubridate::year(Datetime) ) %>%
mutate(hour_t = hour) %>%
group_by(year_t, hour_t) %>%
summarize( across(everything(), ~ mean_na_handling(.x, na_ratio_threshold) ) ) %>%
as_tsibble(key = year_t, index = hour_t) %>%
select(hour_t,year_t,nor,lap,log)
ts_year_hours_NS = ts_month_hours_NS %>%
index_by(year_t = lubridate::year(Datetime) ) %>%
mutate(hour_t = hour) %>%
group_by(year_t, hour_t) %>%
summarize( across(everything(), ~ mean_na_handling(.x, na_ratio_threshold) ) ) %>%
as_tsibble(key = year_t, index = hour_t) %>%
select(hour_t,year_t,nor,lap,log)
# Plots
p1 <- ts_year_hours_EW %>%
ggplot(aes(x = hour_t, y = nor, color = as.factor(year_t) ) ) +
geom_line( size = 1.2, alpha = 0.7 ) +
scale_y_continuous(breaks = seq(0.0, 0.7, 0.1), limits = c(0.0, 0.7), name = "Gaussian Occurrence [%]" ) +
scale_x_continuous(breaks = seq(0,23,1), name = "Time [Hour]" ) +
scale_colour_manual(values = years_color_scale,
name= NULL,
) +
theme(legend.position = "none",
axis.text.x = element_text(size = 8),
axis.title.y = element_text(size = 10)
)
p2 <- ts_year_hours_NS %>%
ggplot(aes(x = hour_t, y = nor, color = as.factor(year_t) ) ) +
geom_line( size = 1.2, alpha = 0.7 ) +
scale_y_continuous(breaks = seq(0.0, 0.7, 0.1), limits = c(0.0, 0.7), name = "Gaussian Occurrence [%]" ) +
scale_x_continuous(breaks = seq(0,23,1), name = "Time [Hour]" ) +
scale_colour_manual(values = years_color_scale,
name= NULL,
) +
theme(legend.position = "none",
axis.text.x = element_text(size = 8),
axis.title.y = element_text(size = 10)
)
prow <- plot_grid(p1, p2, align = "vh", nrow = 1)
legend_dist <- get_legend(p1 + theme(
legend.position="top",
legend.box = "horizontal"
)
)
plot_grid(legend_dist, prow, nrow = 2, rel_heights = c(0.1, 0.90))
#intensity and size of three major thunderstorm centers (from Schumann Resonance for Tyros - Nickolaenko and Hayakawa, 2014)
t_storms = tibble(
hour_t = 0:23,
Africa = c(0.45, 0.51, 0.43, 0.32, 0.24, 0.18, 0.13, 0.14, 0.22, 0.35, 0.55,
0.88, 1.38, 2, 2.4, 2.7, 2.47, 2.03, 1.63, 1.3, 1.02, 0.8, 0.6, 0.51),
America = c(1.75, 1.38, 1.17, 0.91, 0.78, 0.65, 0.52, 0.43, 0.4, 0.35, 0.28,
0.22, 0.18, 0.17, 0.18, 0.28, 0.47, 0.87, 1.42, 1.9, 2.24, 2.45, 2.3, 2.08),
Asia = c(0.08, 0.12, 0.15, 0.22, 0.38, 0.6, 0.88, 1.24, 1.46, 1.65, 1.58,
1.37, 1.14, 0.95, 0.7, 0.6, 0.5, 0.4, 0.3, 0.3, 0.2, 0.15, 0.1, 0.08)
)
p3 <- t_storms %>%
full_join( ts_year_hours_EW %>%
index_by(hour_t) %>%
summarize( nor_EW = mean(nor*4) ), by = "hour_t") %>%
full_join( ts_year_hours_NS %>%
index_by(hour_t) %>%
summarize( nor_NS = mean(nor*4) ), by = "hour_t") %>%
pivot_longer(c(Africa, America, Asia, nor_EW, nor_NS), names_to = "key", values_to ="value") %>%
ggplot( aes( x = hour_t, y = value, color = as.factor(key) ) ) +
geom_line( size = 1.2, alpha = 0.7 ) +
scale_colour_manual(values = c("springgreen", "darkolivegreen4",
"darkgreen", "lightskyblue", "darkblue"),
name= NULL,
breaks = c("Africa", "America", "Asia", "nor_EW", "nor_NS"),
labels = c("Africa (left)", "America (left)", "Asia (left)",
"EW average (right)", "NS average (right)")
) +
scale_y_continuous(breaks = seq(0.0, 3, 0.5), limits = c(0.0, 3), name = "Intensity [AU]",
sec.axis = sec_axis(trans = ~./4, name = "Gaussian Ocurrence [%]")
) +
scale_x_continuous(breaks = seq(0,23,1), name = "Time [Hour]") +
theme(legend.position="top",
legend.box = "horizontal",
legend.spacing.x = unit(0.5, 'cm'),
axis.text.x = element_text(size = 10),
axis.title.y = element_text(size = 10)
)
# spatial parameters of global thunderstorm centers (Schumann Resonance for Tyros).
t_sp = tibble(
Af = c(10, 10, 5, 28, 8, 8),
Am = c(4, 30, -15, -66, 12, -6),
As = c(2.5, 10, 0, 120, 20, 0)
)
#Monthly coordinates of storm centers
lat_Af = mean( -( t_sp$Af[1] + t_sp$Af[2]*cos((1:12+3)*pi/6) + t_sp$Af[3]*cos((1:12+3)*pi/3) ) )
long_Af = mean( t_sp$Af[4] + t_sp$Af[5]*cos((1:12+3)*pi/6) + t_sp$Af[6]*cos((1:12+3)*pi/3) )
lat_Am = mean( -( t_sp$Am[1] + t_sp$Am[2]*cos((1:12+3)*pi/6) + t_sp$Am[3]*cos((1:12+3)*pi/3) ) )
long_Am = mean( t_sp$Am[4] + t_sp$Am[5]*cos((1:12+3)*pi/6) + t_sp$Am[6]*cos((1:12+3)*pi/3) )
lat_As = mean( -( t_sp$As[1] + t_sp$As[2]*cos((1:12+3)*pi/6) + t_sp$As[3]*cos((1:12+3)*pi/3) ) )
long_As = mean( t_sp$As[4] + t_sp$As[5]*cos((1:12+3)*pi/6) + t_sp$As[6]*cos((1:12+3)*pi/3) )
2.907*pi()/180
pi
2.907*pi/180
2.907*180/pi
2.486*180/pi
4.363*180/pi
1.239*180/pi
C_Af = c( abs( sin(2.486) ), abs( cos(2.486) ) )
C_Am = c( abs( sin(4.363) ), abs( cos(4.363) ) )
C_As = c( abs( sin(1.239) ), abs( cos(1.239) ) )
